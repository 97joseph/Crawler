# Python Crawler Scripts
![python version](https://img.shields.io/badge/python-3.5-brightgreen.svg)
### [知乎](https://www.zhihu.com/)|[微博]((https://weibo.com))|[B站](https://www.bilibili.com/)|[豆瓣]((https://www.douban.com/))|[pixiv]((https://www.pixiv.net/))|[贴吧](http://c.tieba.baidu.com/)|[优酷](http://www.youku.com/)|[bing](https://cn.bing.com/)爬虫项目的合集

## 目录
### [豆瓣](#douban)
### [Pixiv](#pixiv_)
### [微博](#weibo)
### [贴吧](#tieba)
### [Bilibili](#bilibili_)
### [优酷](#youku)
### [bing](#bing)
### [知乎](#zhihu)


## <p id="douban">豆瓣爬虫</p>
* [爬取豆瓣电影top250并存到Excel中](https://github.com/LewisTian/Python/blob/master/douban/MovieTop250.py)
![](https://github.com/LewisTian/Python/blob/master/douban/movieTop250.png)

## <p id="pixiv_">Pixiv爬虫</p>
* [爬取pixiv首页的轮换图片](https://github.com/LewisTian/Python/blob/master/pixiv/cover.py)
![](https://github.com/LewisTian/Python/blob/master/pixiv/pixiv.png)

## <p id="weibo">微博爬虫</p>
* [爬取微博亚洲新歌榜top50并存到Excel中](https://github.com/LewisTian/Python/blob/master/weibo/NewSongTop50.py)
![](https://github.com/LewisTian/Python/blob/master/weibo/weibo.png)

## <p id="bilibili_">bilibili爬虫</p>
* [爬取B站视频弹幕并保存到txt中](https://github.com/LewisTian/Python/blob/master/bilibili/danmu.py)
    - 使用方法：python danmu.py https://www.bilibili.com/video/av9933492/

## <p id="tieba">贴吧爬虫</p>
* [爬取贴吧图片并保存到对应pid文件夹下](https://github.com/LewisTian/Python/blob/master/tieba/image.py)
    - 使用方法：python image.py 2271504759

## <p id="youku">优酷爬虫</p>
* [爬取优酷视频弹幕](https://github.com/LewisTian/Python/blob/master/youku/danmu.py)
    - 使用前记得修改data中的数据，我爬取的是火影第一集的弹幕。
    - 使用方法：python danmu.py
    - [video](https://www.bilibili.com/video/av13784309/)

## <p id="bing">bing爬虫</p>
* [爬取bing主页背景图](https://github.com/LewisTian/Python/blob/master/bing/cover.py)
    - <img src="https://i.loli.net/2017/08/31/59a7cde9510a8.png" alt="Screenshot" title="Screenshot" height="150" />
    - <img src="https://cn.bing.com/az/hprichbg/rb/ChamonixClouds_ZH-CN7700889231_1920x1080.jpg" title="download" alt="download" height="150" />

## <p id="zhihu">知乎爬虫</p>
* [爬取知乎回答图片](https://github.com/LewisTian/Python/blob/master/zhihu/image.py)
    - 使用前需要更新问题id, 填入Cookie,include应该不需要更新
    - 使用方法：python image.py


## [to Top](#top)
